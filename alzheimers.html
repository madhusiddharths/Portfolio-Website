<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Alzheimer's Progression</title>
  <link rel="stylesheet" href="css/normalize.css" />
  <link rel="stylesheet" href="css/style.css" />
  <link rel="stylesheet" href="./css/font-root-styles/general-sans.css" />
  <link rel="stylesheet" href="./css/font-root-styles/okinesans.css" />
  <link rel="stylesheet" href="./css/font-root-styles/satoshi.css" />
</head>
<body>

<header>
  <h1>Alzheimer's Progression</h1>
  <div class="separator"></div>
</header>

<main>
  <article>
    <section style="width: 100%; margin: 0 auto;">
      <h2>Overview</h2>
      <p>This project focuses on predicting the progression stages of Alzheimer’s disease using MRI scans. It leverages
        a transfer learning approach based on the VGG19 convolutional neural network to classify the stages of the
        disease accurately. In addition to diagnosis, the system is capable of generating future-stage MRI images of a
        patient's brain using a CycleGAN-based generative architecture, providing visual insight into how the disease
        may evolve over time. The entire solution is deployed as a web application using the Django framework, enabling
        accessible and scalable interaction with the model.</p>
    </section>

    <section style="width: 100%; margin: 0 auto;">
      <h2>Data Preprocessing</h2>
      <p>
        The dataset used for this project is the OASIS MRI dataset, which contains brain scan images categorized
        into four stages of Alzheimer’s disease: No Alzheimer’s, Very Mild Alzheimer’s, Mild Alzheimer’s, and Moderate
        Alzheimer’s. As is common with real-world medical datasets, the distribution is highly imbalanced. Over 60% of
        the images fall under the "No Alzheimer’s" category, around 20% represent "Very Mild Alzheimer’s," approximately
        15% are labeled as "Mild," and fewer than 4,000 images correspond to the "Moderate Alzheimer’s" stage. To
        address this imbalance, we applied a combination of undersampling the dominant class and oversampling the
        minority class. Additionally, the cyclic nature of our model architecture—which employs three GANs to simulate
        progression between stages—further helped in mitigating the effects of data imbalance by generating synthetic
        samples for underrepresented categories. More details about the architecture and its role in addressing this
        challenge will be discussed in the upcoming sections.
      </p>
    </section>


    <section style="width: 100%; margin: 0 auto;">
      <figure style="margin: 2rem 0; text-align: center;">
        <img src="img/brain_diff.png" alt="Difference in mean images between stages"
             style="max-width:600px; width: 100%; height:auto; border:1px solid #ccc; margin-bottom: 1.5rem;" />
        <figcaption style="font-style: italic; color: #666; margin-top: 0.5rem;">
          Figure 1: Difference in mean images between stages.
        </figcaption>
      </figure>
      <p>This image highlights the brain regions most affected as Alzheimer’s progresses, based on differences between
        mean MRI images at each stage. These spatial patterns reveal structural changes—such as cortical thinning and
        ventricular enlargement—that are consistent across patients. By analyzing these differences, we can better
        understand where the disease manifests in the brain. This information can help guide our model’s architecture,
        such as focusing attention on high-variation regions or refining preprocessing steps to emphasize critical areas
        of degeneration./p>
    </section>

    <section style="width: 100%; margin: 0 auto;">
      <figure style="margin: 2rem 0; text-align: center;">
        <img src="img/hist_analysis.png" alt="Pixel Intensity Distribution"
             style="max-width:600px; width: 100%; height:auto; border:1px solid #ccc; margin-bottom: 1.5rem;" />
        <figcaption style="font-style: italic; color: #666; margin-top: 0.5rem;">
          Figure 2: Pixel Intensity distribution.
        </figcaption>
      </figure>
      <p>The CycleGAN architecture relies on pixel-level transformations to generate future-stage brain images.
        However, variations in contrast or brightness across the dataset can lead the model to focus on intensity
        differences rather than the structural changes that are clinically relevant. Notably, when analyzing pixel
        intensity distributions, we observe distinct frequency spikes in the 40–60 and 100–140 ranges for brains
        without dementia. As Alzheimer’s progresses, these distributions tend to flatten, indicating a loss of contrast
        and structural definition in affected brain regions. This shift in pixel frequency highlights the importance
        of normalization during preprocessing to ensure the model learns disease-related anatomical changes rather than
        superficial intensity variations.</p>
    </section>

    <section style="width: 100%; margin: 0 auto;">
      <h2>Architecture</h2>
      <p>The first stage of the pipeline involves data preprocessing, where input MRI images are resized and converted
        to RGB format to match the input requirements of the model. These preprocessed images are then passed to a
        VGG19-based transfer learning model, which classifies each scan into one of the four stages of Alzheimer’s:
        No Alzheimer’s, Very Mild, Mild, or Moderate. Based on this classification, the image is routed to one of three
        GAN models—each trained to simulate progression between consecutive stages (No → Very Mild, Very Mild → Mild,
        Mild → Moderate). These GANs are chained in a cyclic architecture, where the output of one GAN serves as the
        input for the next, enabling the system to generate realistic predictions of how a patient's brain MRI might
        evolve through successive stages of the disease.</p>
    </section>

    <section style="width: 100%; margin: 0 auto;">
      <figure style="margin: 2rem 0; text-align: center;">
        <img src="img/architecture.png" alt="Model architecture"
             style="max-width:600px; width: 100%; height:auto; border:1px solid #ccc; margin-bottom: 1.5rem;" />
        <figcaption style="font-style: italic; color: #666; margin-top: 0.5rem;">
          Figure 3: Model architecture.
        </figcaption>
      </figure>
      <p>The final output includes both the predicted current stage of Alzheimer’s and a series of synthesized MRI
        images illustrating how the brain is expected to progress through the subsequent stages.</p>
    </section>

    <section style="width: 100%; margin: 0 auto;">
      <h2>VGG19 based classification model.</h2>
      <p>
        This architecture uses a pre-trained VGG19 model without its top layers to extract deep image features, with
        the base model’s weights frozen to retain learned patterns. After feature extraction, a global average pooling
        layer reduces the spatial dimensions, followed by batch normalization to improve training stability. Then,
        three fully connected layers with ReLU activation and dropout (50%) are added to learn task-specific
        representations and reduce overfitting. The final layer uses softmax activation to classify MRI images into
        Alzheimer’s stages. The model is compiled with the Adam optimizer and categorical cross-entropy loss to
        optimize multiclass classification accuracy.
      </p>
    </section>

    <section style="width: 100%; margin: 0 auto;">
      <div style="display: flex; justify-content: center; gap: 2rem; margin: 2rem 0;">
        <figure style="flex: 1; text-align: center;">
          <img src="img/tloss.png" alt="Training and Validation loss"
               style="max-width: 100%; height: auto; border: 1px solid #ccc;" />
          <figcaption style="font-style: italic; color: #666; margin-top: 0.5rem;">
            Figure 4a: Training and Validation loss.
          </figcaption>
        </figure>
        <figure style="flex: 1; text-align: center;">
          <img src="img/cl_report.png" alt="Classification report"
               style="max-width: 100%; height: auto; border: 1px solid #ccc;" />
          <figcaption style="font-style: italic; color: #666; margin-top: 0.5rem;">
            Figure 4b: Classification report.
          </figcaption>
        </figure>
      </div>
      <p>
        Figure 4a presents the training and validation loss of the VGG19-based classification model. Although the
        validation loss is roughly twice the training loss, the difference is not large enough to significantly
        impact the model’s performance. Figure 5 displays additional evaluation metrics for the classification model.
      </p>
    </section>

    <section style="width: 100%; margin: 0 auto;">
      <h2>Generative Adversarial Networks (GANs)</h2>
      <p>The architecture incorporates three GANs, each responsible for modeling progression between Alzheimer’s stages:
        from no Alzheimer’s to very mild (no → vmild), very mild to mild (vmild → mild), and mild to moderate (mild →
        moderate). All three GANs share the same underlying architecture.</p>
      <p>The <strong>Generator</strong> follows an encoder-decoder convolutional neural network design. It begins with
        three convolutional layers that downsample the input image (single channel) into 256 feature maps, using a
        kernel size of 4, stride of 2, and padding of 1. Each layer is followed by a ReLU activation, with Batch
        Normalization applied after the second and third layers to enhance training stability. After encoding, the
        model upsamples the features through three transposed convolution layers, gradually reducing the channels from
        256 back to 1. Each upsampling layer also includes Batch Normalization and ReLU, except for the final output
        layer, which uses a Tanh activation to scale pixel values between -1 and 1. This structure enables the Generator
        to learn detailed pixel-level transformations between stages.</p>
      <p>The <strong>Discriminator</strong> acts as a convolutional classifier, taking a single-channel image and
        passing it through four convolutional layers. The first three use kernel size 4, stride 2, and padding 1,
        combined with LeakyReLU activations (negative slope 0.2) and Batch Normalization after the second and third
        layers. These layers extract hierarchical features while reducing spatial dimensions. The final convolutional
        layer, with kernel size 4, stride 1, and no padding, produces a single scalar output after a Sigmoid activation,
        representing the probability that the input image is real versus generated. The output tensor is flattened to
        shape [batch_size], providing a classification score for each image in the batch.</p>
      <p>Both Generator and Discriminator models are deployed on GPU if available and are trained adversarially using
        binary cross-entropy loss. The Generator’s goal is to produce realistic images that fool the Discriminator,
        while the Discriminator learns to better distinguish between real and generated images, resulting in
        progressively improved image generation quality.</p>
    </section>

    <section style="width: 100%; margin: 0 auto;">
      <div style="display: flex; justify-content: space-between; gap: 1rem; margin: 2rem 0;">
        <figure style="flex: 1; text-align: center;">
          <img src="img/input_none.png" alt="No Alzheimer's input"
               style="width: 100%; height: auto; border: 1px solid #ccc;" />
          <figcaption style="font-style: italic; color: #666; margin-top: 0.5rem;">
            Figure 5a: Input (No Alzheimer's).
          </figcaption>
        </figure>
        <figure style="flex: 1; text-align: center;">
          <img src="img/predicted_very_mild_dementia.png" alt="Very Mild Alzheimer's"
               style="width: 100%; height: auto; border: 1px solid #ccc;" />
          <figcaption style="font-style: italic; color: #666; margin-top: 0.5rem;">
            Figure 5b: Very Mild.
          </figcaption>
        </figure>
        <figure style="flex: 1; text-align: center;">
          <img src="img/predicted_mild_dementia.png" alt="Mild Alzheimer's"
               style="width: 100%; height: auto; border: 1px solid #ccc;" />
          <figcaption style="font-style: italic; color: #666; margin-top: 0.5rem;">
            Figure 5c: Mild.
          </figcaption>
        </figure>
        <figure style="flex: 1; text-align: center;">
          <img src="img/predicted_moderate_dementia.png" alt="Moderate Alzheimer's"
               style="width: 100%; height: auto; border: 1px solid #ccc;" />
          <figcaption style="font-style: italic; color: #666; margin-top: 0.5rem;">
            Figure 5d: Moderate.
          </figcaption>
        </figure>
      </div>
    </section>

    <section style="width: 100%; margin: 0 auto;">
      <h2>Region Of Interest (ROI)</h2>
      <p>To further enhance the accuracy of the predicted images, we introduced the concept of Region of Interest (ROI).
        By focusing on specific, clinically relevant areas of the brain, we implemented a region-based discriminator
        that is better equipped to capture subtle structural changes. This approach allows the model to learn finer
        details in the most affected regions, resulting in more realistic and diagnostically meaningful image
        generation.</p>
    </section>

    <section style="width: 100%; margin: 0 auto;">
      <figure style="margin: 2rem 0; text-align: center;">
        <img src="img/roi.png" alt="Outline of the brain to crop ROI for each image."
             style="max-width:600px; width: 100%; height:auto; border:1px solid #ccc; margin-bottom: 1.5rem;" />
        <figcaption style="font-style: italic; color: #666; margin-top: 0.5rem;">
          Figure 6: Outline of the brain to crop ROI for each image.
        </figcaption>
      </figure>
    </section>

    <section style="width: 100%; margin: 0 auto;">
      <figure style="margin: 2rem 0; text-align: center;">
        <img src="img/roi_generated.png" alt="Images generated by ROI."
             style="max-width:600px; width: 100%; height:auto; border:1px solid #ccc; margin-bottom: 1.5rem;" />
        <figcaption style="font-style: italic; color: #666; margin-top: 0.5rem;">
          Figure 7: Images generated by ROI.
        </figcaption>
      </figure>
      <p>At epoch 15, the CycleGAN model trained with ROI (Region of Interest) guidance for the very mild to mild
        Alzheimer's stage transition outperformed the version without ROI. It achieved lower average MSE (0.0326 vs.
        0.0378), higher PSNR (14.93 dB vs. 14.33 dB), and slightly better SSIM (0.3330 vs. 0.3295). It also
        produced more perceptually similar results with a lower LPIPS (0.3092 vs. 0.3191) and significantly improved
        FID (30.94 vs. 39.99), indicating sharper and more realistic outputs when ROI information was used.</p>
    </section>

    <section style="width: 100%; margin: 0 auto;">
      <h2>Conclusion</h2>
      <p>This project offered deep insights into the workings of Generative Adversarial Networks (GANs), transfer
        learning, and web deployment using Django. The key technologies used were TensorFlow for building the
        VGG19-based classification model, PyTorch for implementing the GANs, and Django for deploying the application.
        A significant learning outcome was understanding the challenges of compressing and reconstructing information
        within encoder-decoder architectures—particularly the bottleneck effect that can lead to loss of critical
        details in GAN-generated images. Extending similar approaches to other medical datasets, such as those related
        to cancer detection and progression, holds great potential for advancing targeted treatment and precision
        medicine in clinical practice.</p>
    </section>
  </article>
</main>
<footer>
  <p>Designed and Developed by <span>Madhu Siddharth Suthagar</span></p>
</footer>
<script type="text/javascript" src="js/main.js"></script>
</body>
</html>
